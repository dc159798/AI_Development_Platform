{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "  # Lowercase text\n",
    "  text = text.lower()\n",
    "  # Remove special characters\n",
    "  text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "  # Remove extra whitespace\n",
    "  text = re.sub(r\"\\s+\", \" \", text)\n",
    "  return text\n",
    "\n",
    "def preprocess_data(data):\n",
    "  # Load data (replace with your data loading logic)\n",
    "  data = pd.read_csv(\"data.csv\")\n",
    "  \n",
    "  # Clean text data in a specific column\n",
    "  data[\"text_column\"] = data[\"text_column\"].apply(clean_text)\n",
    "  \n",
    "  # Handle missing values (e.g., impute or remove rows)\n",
    "  data.dropna(subset=[\"text_column\"], inplace=True)\n",
    "  \n",
    "  # Tokenize text (consider NLTK or spaCy based on your needs)\n",
    "  # ... (implementation using chosen library)\n",
    "  \n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing PDF: [Errno 21] Is a directory: '/home/haxck/Desktop/AI_Development_Platform/data'\n",
      "Failed to extract tokens from PDF.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/haxck/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import PyPDF2\n",
    "\n",
    "# Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_pdf(pdf_path):\n",
    "  \"\"\"\n",
    "  Attempts to extract text from a PDF and perform tokenization using NLTK.\n",
    "\n",
    "  **Limitations:**\n",
    "  * This approach relies on PyPDF2 which might not handle complex PDFs well.\n",
    "  * Consider using dedicated PDF extraction libraries for better results.\n",
    "\n",
    "  Args:\n",
    "      pdf_path (str): Path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "      list: List of tokens (words) extracted from the PDF.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Open PDF with PyPDF2\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "      pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "      # Extract text (might be inaccurate for complex PDFs)\n",
    "      text = \"\"\n",
    "      for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "      # Tokenize text using NLTK\n",
    "      tokens = nltk.word_tokenize(text)\n",
    "      return tokens\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing PDF: {e}\")\n",
    "    return []\n",
    "\n",
    "# Example usage (replace with your PDF path)\n",
    "pdf_path = \"/home/haxck/Desktop/AI_Development_Platform/data\"\n",
    "tokens = tokenize_pdf(pdf_path)\n",
    "\n",
    "if tokens:\n",
    "  print(\"Extracted tokens:\")\n",
    "  print(tokens)\n",
    "else:\n",
    "  print(\"Failed to extract tokens from PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from transformers import TFBertModel, BertTokenizer  # Assuming you choose Bert-based LLM (Mistral-7B or Llama2))\n",
    "\n",
    "# Load pre-trained model and tokenizer (replace with specific model names)\n",
    "model_name = \"bert-base-uncased\"  # Replace with chosen LLM identifier\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Define your data pre-processing logic (using TensorFlow Text or other libraries)\n",
    "# ... (your data cleaning and tokenization code)\n",
    "\n",
    "# Define fine-tuning hyperparameters (learning rate, batch size, epochs)\n",
    "learning_rate = ...\n",
    "batch_size = ...\n",
    "epochs = ...\n",
    "\n",
    "# Define fine-tuning model (potentially with additional layers on top of the pre-trained model)\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int32)\n",
    "embeddings = model(input_ids)[0]  # Extract token embeddings\n",
    "# Add additional layers if needed (e.g., for specific task)\n",
    "output = Dense(num_classes, activation=\"softmax\")(embeddings)  # Assuming classification task\n",
    "model = tf.keras.Model(inputs=input_ids, outputs=output)\n",
    "\n",
    "# Compile the model for training\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "# Train the model on your pre-processed data\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Save the fine-tuned model (weights and configuration)\n",
    "# ... (your model saving logic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 05:45:25.670167: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-13 05:45:29.042248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFBertTokenizer, TFBertForSequenceClassification\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, UploadFile, File\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertTokenizer, TFBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Function to load Mistral-7B model and tokenizer\n",
    "def load_mistral_7b():\n",
    "  \"\"\"Loads Mistral-7B tokenizer and model from Hugging Face.\"\"\"\n",
    "  model_name = \"allenai/mistral-base\"  # Replace with actual Mistral-7B identifier (if different)\n",
    "  tokenizer = TFBertTokenizer.from_pretrained(model_name)\n",
    "  model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "  return tokenizer, model\n",
    "\n",
    "# Initialize tokenizer and model (using the defined function)\n",
    "tokenizer, model = load_mistral_7b()\n",
    "\n",
    "# Data Upload and Preprocessing\n",
    "@app.post(\"/upload/\")\n",
    "async def upload_file(file: UploadFile = File(...)):\n",
    "  # Save the uploaded file\n",
    "  with open(file.filename, \"wb\") as buffer:\n",
    "    buffer.write(await file.read())\n",
    "\n",
    "  # Load and preprocess the data (assuming CSV format)\n",
    "  data = pd.read_csv(file.filename)\n",
    "  data = clean_data(data)\n",
    "  data_tokenized = tokenize_data(data, tokenizer)\n",
    "\n",
    "  return {\"message\": \"Data uploaded and preprocessed successfully\"}\n",
    "\n",
    "def clean_data(data):\n",
    "  # Implement data cleaning logic here\n",
    "  return data\n",
    "\n",
    "def tokenize_data(data, tokenizer):\n",
    "  # Implement tokenization logic using the tokenizer\n",
    "  # ... (your data tokenization code)\n",
    "  return data_tokenized\n",
    "\n",
    "# Training and Fine-tuning\n",
    "@app.post(\"/train/\")\n",
    "async def train_model():\n",
    "  # Load preprocessed data (assuming stored after upload)\n",
    "  data = pd.read_csv(\"preprocessed_data.csv\")  # Replace with your data path\n",
    "  labels = data[\"label_column\"]  # Assuming a label column exists\n",
    "  inputs = data.drop(\"label_column\", axis=1)  # Assuming label column separation\n",
    "\n",
    "  # Data splitting\n",
    "  train_inputs, temp_data, train_labels, temp_labels = train_test_split(\n",
    "      inputs, labels, test_size=0.3, random_state=42\n",
    "  )\n",
    "  val_inputs, test_inputs, val_labels, test_labels = train_test_split(\n",
    "      temp_data, temp_labels, test_size=0.5, random_state=42\n",
    "  )\n",
    "\n",
    "  # Prepare datasets for training (TensorFlow format)\n",
    "  train_encodings = tokenizer(\n",
    "      train_inputs.to_list(), return_tensors=\"tf\", padding=\"max_length\", truncation=True\n",
    "  )\n",
    "  val_encodings = tokenizer(\n",
    "      val_inputs.to_list(), return_tensors=\"tf\", padding=\"max_length\", truncation=True\n",
    "  )\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices((train_encodings[\"input_ids\"], train_labels))\n",
    "  val_dataset = tf.data.Dataset.from_tensor_slices((val_encodings[\"input_ids\"], val_labels))\n",
    "\n",
    "  # Define training parameters (can be adjusted)\n",
    "  learning_rate = 2e-5\n",
    "  batch_size = 8\n",
    "  epochs = 3\n",
    "\n",
    "  # Fine-tuning model (potentially with additional layers on top)\n",
    "  # ... (model definition similar to previous examples using TensorFlow)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[\"accuracy\"],\n",
    "  )\n",
    "\n",
    "  # Early stopping (optional)\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "\n",
    "  # Model checkpointing (optional)\n",
    "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "  )\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(\n",
    "      train_dataset.batch(batch_size),\n",
    "      epochs=epochs,\n",
    "      validation_data=val_dataset.batch(batch_size),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e7e9820bb040a5b6f49141cc7e54d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae38a9e56dcf4cd09c5939ef28f7f7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae3d4fd72a6456f95b7cbb38972edb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503f461c5bbd4d6088500e437b81ddd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998701810836792}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('I love programming!'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
